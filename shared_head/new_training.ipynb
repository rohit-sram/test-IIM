{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c5eda7",
   "metadata": {},
   "source": [
    "### Training the new model with additional dataset - Larch Casebearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eca89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 24922\n",
      "Missing files: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# paths\n",
    "CSV_PATH = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\\overall_merged_images.csv\")\n",
    "BASE_DIR = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\")\n",
    "\n",
    "# load\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert \"image\" in df.columns, \"CSV must have a column named 'image'\"\n",
    "\n",
    "# build absolute paths\n",
    "df[\"abs_path\"] = df[\"image\"].apply(lambda p: (BASE_DIR / Path(str(p).replace(\"\\\\\",\"/\"))).resolve())\n",
    "df[\"exists\"] = df[\"abs_path\"].apply(lambda p: p.exists())\n",
    "\n",
    "# summary\n",
    "missing = df.loc[~df[\"exists\"], \"image\"]\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing files: {len(missing)}\")\n",
    "if len(missing) > 0:\n",
    "    print(\"Examples of missing paths:\")\n",
    "    print(missing.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb6d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe108595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOTS = {\n",
    "    \"vedai\": Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "    \"shha\":  Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "    \"shhb\":  Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "    \"larch\": Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "}\n",
    "\n",
    "MODEL_NAME   = \"tiny_vit_5m_224.dist_in22k_ft_in1k\"\n",
    "IMG_SIZE     = 224\n",
    "NUM_CLASSES  = 3\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 10\n",
    "LR           = 2e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE     = 5                 # early stopping patience (epochs)\n",
    "LABEL_SMOOTH = 0.05              # label smoothing for CE\n",
    "SEED         = 123\n",
    "NUM_WORKERS  = 2                 # 0 on Windows if needed\n",
    "N_SPLITS     = 5                 # stratified K-fold (set to 1 for a single split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.cuda.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config, create_transform\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43af095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unresolved rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>brightness</th>\n",
       "      <th>edge_density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>abs_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VEDAI_dataset\\VEDAI_512\\images\\00000000_co.png</td>\n",
       "      <td>0</td>\n",
       "      <td>148.686729</td>\n",
       "      <td>0.132713</td>\n",
       "      <td>6.791240</td>\n",
       "      <td>C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VEDAI_dataset\\VEDAI_512\\images\\00000000_ir.png</td>\n",
       "      <td>0</td>\n",
       "      <td>194.854771</td>\n",
       "      <td>0.026806</td>\n",
       "      <td>5.643956</td>\n",
       "      <td>C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VEDAI_dataset\\VEDAI_512\\images\\00000000_ir_tri...</td>\n",
       "      <td>0</td>\n",
       "      <td>181.481246</td>\n",
       "      <td>0.041693</td>\n",
       "      <td>5.920810</td>\n",
       "      <td>C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label  brightness  \\\n",
       "0     VEDAI_dataset\\VEDAI_512\\images\\00000000_co.png      0  148.686729   \n",
       "1     VEDAI_dataset\\VEDAI_512\\images\\00000000_ir.png      0  194.854771   \n",
       "2  VEDAI_dataset\\VEDAI_512\\images\\00000000_ir_tri...      0  181.481246   \n",
       "\n",
       "   edge_density   entropy                                           abs_path  \n",
       "0      0.132713  6.791240  C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...  \n",
       "1      0.026806  5.643956  C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...  \n",
       "2      0.041693  5.920810  C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"image\",\"label\"}.issubset(df.columns), \"CSV must have columns: image,label\"\n",
    "\n",
    "def which_ds(p: str) -> str:\n",
    "    s = str(p).lower().replace(\"\\\\\",\"/\")\n",
    "    if \"vedai_512\" in s or \"vedai_1024\" in s or \"/vedai\" in s:\n",
    "        return \"vedai\"\n",
    "    if \"shhb\" in s or \"/shb\" in s or \"shanghaitech data\\\\shhb\" in s.lower():\n",
    "        return \"shhb\"\n",
    "    if \"shha\" in s or \"/sha\" in s or \"shanghaitech data\\\\shha\" in s.lower():\n",
    "        return \"shha\"\n",
    "    if \"data_set_larch_casebearer\" in s or \"larch_casebearer\" in s or \"augmented_images\" in s or \"/larch/\" in s:\n",
    "        return \"larch\"\n",
    "    return \"vedai\"\n",
    "\n",
    "def resolve_abs(rel: str) -> Path | None:\n",
    "    rel_p = Path(str(rel).replace(\"\\\\\",\"/\"))\n",
    "    # try inferred root first\n",
    "    root = ROOTS.get(which_ds(rel_p.as_posix()), ROOTS[\"vedai\"])\n",
    "    cand = (root / rel_p)\n",
    "    if cand.exists(): return cand.resolve()\n",
    "    # last resort: try all roots\n",
    "    for r in ROOTS.values():\n",
    "        c = (r / rel_p)\n",
    "        if c.exists(): return c.resolve()\n",
    "    return None\n",
    "\n",
    "df[\"abs_path\"] = df[\"image\"].apply(resolve_abs)\n",
    "missing = df[\"abs_path\"].isna().sum()\n",
    "print(\"Unresolved rows:\", missing)\n",
    "if missing:\n",
    "    display(df[df[\"abs_path\"].isna()].head(10))\n",
    "df = df.dropna(subset=[\"abs_path\"]).reset_index(drop=True)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6f3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " label\n",
      "0    13983\n",
      "1     4791\n",
      "2     6148\n",
      "Name: count, dtype: int64\n",
      "Class ratios:\n",
      " label\n",
      "0    0.5611\n",
      "1    0.1922\n",
      "2    0.2467\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "counts = df[\"label\"].value_counts().sort_index()\n",
    "print(\"Class counts:\\n\", counts)\n",
    "print(\"Class ratios:\\n\", (counts / counts.sum()).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a40ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved input_size: (3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "_tmp = timm.create_model('tiny_vit_5m_224.dist_in22k_ft_in1k', pretrained=True, num_classes=NUM_CLASSES)\n",
    "data_cfg = resolve_data_config(model=_tmp)\n",
    "print(\"Resolved input_size:\", data_cfg.get(\"input_size\"))\n",
    "\n",
    "train_tfms = create_transform(**data_cfg, is_training=True)   # includes resize/crop/norm\n",
    "val_tfms   = create_transform(**data_cfg, is_training=False)  # resize + center crop + norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5688291",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = timm.create_model('tiny_vit_5m_224.dist_in22k_ft_in1k', pretrained=True, num_classes=NUM_CLASSES)\n",
    "data_cfg = resolve_data_config(model=_tmp)\n",
    "print(\"Resolved input_size:\", data_cfg.get(\"input_size\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d06d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, transform=None):\n",
    "        self.f = frame.reset_index(drop=True)\n",
    "        self.t = transform\n",
    "    def __len__(self): return len(self.f)\n",
    "    def __getitem__(self, i):\n",
    "        p = Path(self.f.loc[i, \"abs_path\"])\n",
    "        y = int(self.f.loc[i, \"label\"])\n",
    "        x = Image.open(p).convert(\"RGB\")\n",
    "        if self.t: x = self.t(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99b5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=PATIENCE, mode=\"min\", min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.count = 0\n",
    "\n",
    "    def step(self, value) -> bool:\n",
    "        if self.best is None:\n",
    "            self.best = value\n",
    "            return False\n",
    "        improved = (value < self.best - self.min_delta) if self.mode == \"min\" else (value > self.best + self.min_delta)\n",
    "        if improved:\n",
    "            self.best = value\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "        return self.count > self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def00ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES)\n",
    "    return model.to(device)\n",
    "\n",
    "def make_loaders(df_train, df_val):\n",
    "    ds_tr = CSVDataset(df_train, transform=train_tfms)\n",
    "    ds_va = CSVDataset(df_val,   transform=val_tfms)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return dl_tr, dl_va\n",
    "\n",
    "def train_one_model(df_train, df_val, max_epochs=EPOCHS, run_name=\"run\"):\n",
    "    dl_tr, dl_va = make_loaders(df_train, df_val)\n",
    "    model = create_model()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    # Cosine schedule over epochs (per-step cosine also OK; keep simple here)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    es = EarlyStopper(patience=PATIENCE, mode=\"min\")\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    def run_epoch(loader, train=True):\n",
    "        if train: model.train()\n",
    "        else:     model.eval()\n",
    "        tot_loss, tot, correct = 0.0, 0, 0\n",
    "        pbar = tqdm(loader, leave=False)\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            with autocast(True):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            tot_loss += loss.item() * xb.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            tot += xb.size(0)\n",
    "        return tot_loss / max(tot,1), correct / max(tot,1)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        tr_loss, tr_acc = run_epoch(dl_tr, train=True)\n",
    "        va_loss, va_acc = run_epoch(dl_va, train=False)\n",
    "        scheduler.step()\n",
    "        print(f\"[{run_name}] epoch {epoch:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = { \"state_dict\": model.state_dict(),\n",
    "                           \"num_classes\": NUM_CLASSES,\n",
    "                           \"img_size\": IMG_SIZE }\n",
    "        if es.step(va_loss):\n",
    "            print(f\"Early stopping at epoch {epoch} (best val loss {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Restore best state for evaluation\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"state_dict\"])\n",
    "    return model, best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c7aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = 4        # try 2 if RAM is tight\n",
    "PREFETCH    = 2\n",
    "BATCH_SIZE  = 32       # adjust if VRAM limited\n",
    "MAX_TRAIN_STEPS = 200 # set e.g. 200 for quick feedback, else None\n",
    "MAX_VAL_STEPS   = 60 # set e.g. 60  for quick feedback, else None\n",
    "\n",
    "def make_loaders(df_train, df_val):\n",
    "    ds_tr = CSVDataset(df_train, transform=train_tfms)\n",
    "    ds_va = CSVDataset(df_val,   transform=val_tfms)\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=PREFETCH\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        ds_va, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=PREFETCH\n",
    "    )\n",
    "    return dl_tr, dl_va\n",
    "\n",
    "# One-line device sanity once you create the model in training:\n",
    "# print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 | train=19937 val=4985 ===\n",
      "train label counts: {0: 11186, 1: 3833, 2: 4918}\n",
      "val   label counts: {0: 2797, 1: 958, 2: 1230}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/624 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_ALL_FOLDS = False\n",
    "labels = df[\"label\"].values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_models = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):\n",
    "    df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "    df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n=== Fold {fold}/{N_SPLITS} | train={len(df_tr)} val={len(df_va)} ===\")\n",
    "    print(\"train label counts:\", df_tr[\"label\"].value_counts().sort_index().to_dict())\n",
    "    print(\"val   label counts:\", df_va[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "    model, best_state = train_one_model(df_tr, df_va, max_epochs=EPOCHS, run_name=f\"fold{fold}\")\n",
    "    fold_models.append((model, best_state))\n",
    "\n",
    "    if not TRAIN_ALL_FOLDS:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0ca47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0666e99",
   "metadata": {},
   "source": [
    "preivious k-fold training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_ALL_FOLDS = False  # set True to train all folds\n",
    "\n",
    "labels = df[\"label\"].values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_models = []\n",
    "if __name__ == \"__main__\":  # important on Windows to allow num_workers>0 in your dataloaders\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):\n",
    "        df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "        df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\n=== Fold {fold}/{N_SPLITS} | train={len(df_tr)} val={len(df_va)} ===\")\n",
    "        print(\"train label counts:\", df_tr[\"label\"].value_counts().sort_index().to_dict())\n",
    "        print(\"val   label counts:\", df_va[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "        model, best_state = train_one_model(df_tr, df_va, max_epochs=EPOCHS, run_name=f\"fold{fold}\")\n",
    "        fold_models.append((model, best_state))\n",
    "\n",
    "        if not TRAIN_ALL_FOLDS:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 | train=19937 val=4985 ===\n",
      "train label counts: {0: 11186, 1: 3833, 2: 4918}\n",
      "val   label counts: {0: 2797, 1: 958, 2: 1230}\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/624 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import numpy as np\n",
    "# from torch.amp import GradScaler\n",
    "# from torch import nn, autocast\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# TRAIN_ALL_FOLDS = False  # True = train all folds; False = first fold only\n",
    "# labels = df[\"label\"].values\n",
    "# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# def train_one_model(df_train, df_val, max_epochs=EPOCHS, run_name=\"run\"):\n",
    "#     dl_tr, dl_va = make_loaders(df_train, df_val)\n",
    "#     model = create_model()\n",
    "#     print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "#     scaler = GradScaler()\n",
    "#     es = EarlyStopper(patience=PATIENCE, mode=\"min\")\n",
    "#     best_val = float(\"inf\")\n",
    "#     best_state = None\n",
    "\n",
    "#     def run_epoch(loader, train=True):\n",
    "#         model.train(train)\n",
    "#         total_loss, correct, total = 0.0, 0, 0\n",
    "#         step = 0\n",
    "#         pbar = tqdm(loader, leave=False)\n",
    "#         for xb, yb in pbar:\n",
    "#             step += 1\n",
    "#             xb = xb.to(\"cuda\", non_blocking=True) if torch.cuda.is_available() else xb\n",
    "#             yb = yb.to(\"cuda\", non_blocking=True) if torch.cuda.is_available() else yb\n",
    "#             with autocast(True):\n",
    "#                 logits = model(xb)\n",
    "#                 loss = criterion(logits, yb)\n",
    "#             if train:\n",
    "#                 optimizer.zero_grad(set_to_none=True)\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#             total_loss += loss.item() * xb.size(0)\n",
    "#             correct += (logits.argmax(1) == yb).sum().item()\n",
    "#             total   += xb.size(0)\n",
    "#             if train and MAX_TRAIN_STEPS and step >= MAX_TRAIN_STEPS: break\n",
    "#             if not train and MAX_VAL_STEPS and step >= MAX_VAL_STEPS: break\n",
    "#         return total_loss / max(total,1), correct / max(total,1)\n",
    "\n",
    "#     for epoch in range(1, max_epochs+1):\n",
    "#         tr_loss, tr_acc = run_epoch(dl_tr, train=True)\n",
    "#         va_loss, va_acc = run_epoch(dl_va, train=False)\n",
    "#         scheduler.step()\n",
    "#         print(f\"[{run_name}] epoch {epoch:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "#         if va_loss < best_val:\n",
    "#             best_val = va_loss\n",
    "#             best_state = {\"state_dict\": model.state_dict(), \"num_classes\": NUM_CLASSES, \"img_size\": IMG_SIZE}\n",
    "#         if es.step(va_loss):\n",
    "#             print(f\"Early stopping at epoch {epoch} (best val loss {best_val:.4f})\")\n",
    "#             break\n",
    "\n",
    "#     if best_state is not None:\n",
    "#         model.load_state_dict(best_state[\"state_dict\"])\n",
    "#     return model, best_state\n",
    "\n",
    "# fold_models = []\n",
    "# if __name__ == \"__main__\":\n",
    "#     for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):\n",
    "#         df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "#         df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "#         print(f\"\\n=== Fold {fold}/{N_SPLITS} | train={len(df_tr)} val={len(df_va)} ===\")\n",
    "#         print(\"train label counts:\", df_tr[\"label\"].value_counts().sort_index().to_dict())\n",
    "#         print(\"val   label counts:\", df_va[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "#         model, best_state = train_one_model(df_tr, df_va, max_epochs=EPOCHS, run_name=f\"fold{fold}\")\n",
    "#         fold_models.append((model, best_state))\n",
    "\n",
    "#         if not TRAIN_ALL_FOLDS:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903daabc",
   "metadata": {},
   "source": [
    "### COMPLETElY CHANGING THE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf35667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f032e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CSV: C:\\Users\\rsriram3\\Documents\\ind_study\\data\\overall_merged_images.csv\n",
      "ROOT_DIR: C:\\Users\\rsriram3\\Documents\\ind_study\\data\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: config & imports (edit paths to match your environment)\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autocast\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "import random\n",
    "\n",
    "# === EDIT THESE PATHS ===\n",
    "CSV_PATH = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\\overall_merged_images.csv\")# your merged csv\n",
    "ROOT_DIR   = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\")  # base path to prepend to 'image' column entries\n",
    "OUTPUT_DIR = ROOT_DIR / \"results\" # where to put models/figures/folds\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === HYPERPARAMS (as in text file) ===\n",
    "BATCH_SIZE    = 32\n",
    "EPOCHS        = 5\n",
    "LR            = 1e-5 # 3e-5\n",
    "WEIGHT_DECAY  = 1e-2 # 5e-3\n",
    "ALPHA_MIXUP   = 0.2\n",
    "USE_MIXUP     = False           # set True to enable mixup (matches text flow)\n",
    "NUM_FOLDS     = 5\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENCE      = 2               # early stopping patience used in text\n",
    "LABEL_SMOOTH  = 0.08\n",
    "SEED          = 173\n",
    "NUM_WORKERS = 0 # 4 # 2 or 4\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CSV:\", CSV_PATH)\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4dc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: creating transforms of data\n",
    "from timm.data import create_transform\n",
    "\n",
    "IMG_SIZE = 224  # keep your current input size\n",
    "\n",
    "_train_tfms = create_transform(\n",
    "    input_size=(3, IMG_SIZE, IMG_SIZE),\n",
    "    is_training=True,\n",
    "    auto_augment='rand-m7-mstd0.5-inc1',\n",
    "    re_prob=0.15, re_mode='pixel', re_count=1,\n",
    "    color_jitter=0.3,\n",
    "    interpolation='bicubic'\n",
    ")\n",
    "\n",
    "_val_tfms = create_transform(\n",
    "    input_size=(3, IMG_SIZE, IMG_SIZE),\n",
    "    is_training=False,\n",
    "    interpolation='bicubic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359cdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: dataset, collate, mixup (copied/adjusted from test_text.txt)  :contentReference[oaicite:1]{index=1}\n",
    "import torch\n",
    "\n",
    "# class SharedHeadDataset(Dataset):\n",
    "#     def __init__(self, csv_path, root_dir):\n",
    "#         self.data = pd.read_csv(csv_path)\n",
    "#         self.root_dir = Path(root_dir)\n",
    "#         self.transform = torch.nn.Sequential()  # placeholder; we will use PIL->tensor manually\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.data.iloc[idx]\n",
    "#         img_path = self.root_dir / row['image']\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         # Resize + ToTensor + Normalize: use simple resize & to-tensor here (the original text used Resize->ToTensor)\n",
    "#         image = image.resize((224, 224))\n",
    "#         image = np.array(image).astype(np.float32) / 255.0\n",
    "#         image = torch.from_numpy(image).permute(2,0,1).contiguous()  # C,H,W\n",
    "\n",
    "#         aux = torch.tensor([\n",
    "#             float(row['brightness']), \n",
    "#             float(row['edge_density']), \n",
    "#             float(row['entropy'])\n",
    "#         ], dtype=torch.float32)\n",
    "        \n",
    "#         label = torch.tensor(int(row['label']), dtype=torch.long)\n",
    "#         return image, aux, label\n",
    "\n",
    "'''  NEW  '''\n",
    "class SharedHeadDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir, transform_img=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform_img = transform_img  # NEW: external transform callable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = self.root_dir / row['image']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform_img is not None:\n",
    "            # timm transform expects PIL -> tensor, normalized\n",
    "            image = self.transform_img(image)\n",
    "        else:\n",
    "            # fallback to your previous simple pipeline (kept for safety)\n",
    "            image = image.resize((224, 224))\n",
    "            image = np.array(image).astype(np.float32) / 255.0\n",
    "            image = torch.from_numpy(image).permute(2,0,1).contiguous()\n",
    "\n",
    "        aux = torch.tensor([\n",
    "            float(row['brightness']),\n",
    "            float(row['edge_density']),\n",
    "            float(row['entropy'])\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        label = torch.tensor(int(row['label']), dtype=torch.long)\n",
    "        return image, aux, label\n",
    "\n",
    "\n",
    "def collate_fun(batch):\n",
    "    images, auxs, labels = zip(*batch)\n",
    "    images = torch.stack(images)    # (B,3,224,224)\n",
    "    auxs   = torch.stack(auxs)      # (B,3)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return images, auxs, labels\n",
    "\n",
    "def mixup_data(x, aux, y, alpha=ALPHA_MIXUP):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_aux = lam * aux + (1 - lam) * aux[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, mixed_aux, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e86023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: ExtendedModel + LabelSmoothingCrossEntropy (from text)  :contentReference[oaicite:2]{index=2}\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(self.smoothing / (pred.size(1) - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "# ExtendedModel: uses timm backbone, aux head, fused head (as in test_text)\n",
    "class ExtendedModel(nn.Module):\n",
    "    def __init__(self, backbone_name, num_classes=3):   # set 3 classes for your new dataset\n",
    "        super().__init__()\n",
    "        # self.backbone = create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        \n",
    "        '''  NEW  '''\n",
    "        self.backbone = create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\",\n",
    "            drop_rate=0.1,\n",
    "            drop_path_rate=0.1\n",
    "        )\n",
    "\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(self.backbone.num_features + 16),\n",
    "            nn.Linear(self.backbone.num_features + 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, aux):\n",
    "        # x: (B,3,H,W)\n",
    "        image_features = self.backbone(x)  # (B, D)\n",
    "        aux_features = self.aux_head(aux)  # (B, 16)\n",
    "        combined = torch.cat([image_features, aux_features], dim=1)\n",
    "        return self.head(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daa1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: eval, plotting and EarlyStopping (copied from text)  :contentReference[oaicite:3]{index=3}\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, aux, labels in dataloader:\n",
    "            images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "            outputs = model(images, aux)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    return acc, f1, cm, all_preds, all_labels\n",
    "\n",
    "def plot_metrics(train_accs, val_accs, model_name, save_path):\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(train_accs, label=\"Train Acc\")\n",
    "    plt.plot(val_accs, label=\"Val Acc\")\n",
    "    plt.title(f\"Accuracy vs Epoch - {model_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path / f\"{model_name}_accuracy.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, show=True):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EarlyStopping + train_model (matches test_text.txt training loop)  :contentReference[oaicite:4]{index=4}\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.001, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None or score > self.best_score + self.delta:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# train_model: uses ExtendedModel, optional mixup, label smoothing CE, GradScaler, scheduler, early stopping\n",
    "def train_model(model_name, train_csv, val_csv, root_dir, save_path,\n",
    "                device=DEVICE, use_mixup=USE_MIXUP, label_smoothing=LABEL_SMOOTH):\n",
    "    # train_ds = SharedHeadDataset(train_csv, root_dir)\n",
    "    # val_ds   = SharedHeadDataset(val_csv, root_dir)\n",
    "\n",
    "    # train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fun)\n",
    "    # val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, collate_fn=collate_fun)\n",
    "    \n",
    "    '''  NEW  '''\n",
    "    train_ds = SharedHeadDataset(train_csv, root_dir, transform_img=_train_tfms)\n",
    "    val_ds   = SharedHeadDataset(val_csv,   root_dir, transform_img=_val_tfms)\n",
    "    \n",
    "    pw = NUM_WORKERS > 0\n",
    "    extra = dict(persistent_workers=pw)\n",
    "    if NUM_WORKERS > 0:\n",
    "        extra.update(prefetch_factor=2)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fun, \n",
    "        num_workers=NUM_WORKERS, pin_memory=True, **extra\n",
    "    )\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    xb, xa, yb = next(iter(train_loader))\n",
    "    print(f\"Time to first batch: {time.time() - t0:.2f}s | batch shape: {tuple(xb.shape)}\")\n",
    "    del xb, xa, yb\n",
    "    \n",
    "    val_loader   = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fun, \n",
    "        num_workers=NUM_WORKERS, pin_memory=True, **extra\n",
    "    )\n",
    "\n",
    "\n",
    "    model = ExtendedModel(model_name, num_classes=3).to(device)\n",
    "    ''' NEW '''\n",
    "    ema = ModelEmaV2(model, decay=0.9999)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=1)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE, delta=0.001, verbose=True)\n",
    "    early_stop_path = CHECKPOINT_DIR / f\"{model_name}_earlystop_best.pth\"\n",
    "\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
    "\n",
    "    train_accs, val_accs, train_losses, val_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        total_train_loss = 0.0\n",
    "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for images, aux, labels in progress:\n",
    "            images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mixup:\n",
    "                images, aux_features, targets_a, targets_b, lam = mixup_data(images, aux, labels, ALPHA_MIXUP)\n",
    "                mixup_mode = True\n",
    "            else:\n",
    "                aux_features = aux\n",
    "                mixup_mode = False\n",
    "\n",
    "            with autocast(device_type=str(device).split(':')[0] if device.type=='cuda' else 'cpu'):\n",
    "                outputs = model(images, aux_features)\n",
    "                if mixup_mode:\n",
    "                    loss = mixup_criterion(F.cross_entropy, outputs, targets_a, targets_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            '''  NEW  '''\n",
    "            ema.update(model)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            progress.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        model_for_eval = ema.module   # NEW\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, aux, labels in val_loader:\n",
    "                images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "                # outputs = model(images, aux) \n",
    "                outputs = model_for_eval(images, aux)  # NEW\n",
    "                total_val_loss += F.cross_entropy(outputs, labels).item()  # plain CE for validation\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc, val_f1, cm, _, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        early_stopper(val_loss, model, early_stop_path)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(epoch)\n",
    "        gc.collect()\n",
    "\n",
    "    # if early_stopper.early_stop:\n",
    "    #     model.load_state_dict(torch.load(early_stop_path, map_location=device))\n",
    "\n",
    "    # model.load_state_dict(ema.module.state_dict())   # NEW - overwrite with EMA weights\n",
    "\n",
    "    # # Save last state and reload best if early stopped\n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # return model, train_accs, val_accs, train_losses, val_losses\n",
    "    model.load_state_dict(ema.module.state_dict())   # NEW - overwrite with EMA weights\n",
    "    if early_stopper.early_stop:\n",
    "        model.load_state_dict(torch.load(early_stop_path, map_location=device))\n",
    "\n",
    "    # Save last state\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    return model, train_accs, val_accs, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3cbe5",
   "metadata": {},
   "source": [
    "cell 5 - better saving code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca9eb8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EarlyStopping + train_model (matches test_text.txt training loop)  :contentReference[oaicite:4]{index=4}\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.001, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None or score > self.best_score + self.delta:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# train_model: uses ExtendedModel, optional mixup, label smoothing CE, GradScaler, scheduler, early stopping\n",
    "def train_model(model_name, train_csv, val_csv, root_dir, save_path,\n",
    "                device=DEVICE, use_mixup=USE_MIXUP, label_smoothing=LABEL_SMOOTH):\n",
    "    # train_ds = SharedHeadDataset(train_csv, root_dir)\n",
    "    # val_ds   = SharedHeadDataset(val_csv, root_dir)\n",
    "\n",
    "    # train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fun)\n",
    "    # val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, collate_fn=collate_fun)\n",
    "    \n",
    "    '''  NEW  '''\n",
    "    train_ds = SharedHeadDataset(train_csv, root_dir, transform_img=_train_tfms)\n",
    "    val_ds   = SharedHeadDataset(val_csv,   root_dir, transform_img=_val_tfms)\n",
    "    \n",
    "    pw = NUM_WORKERS > 0\n",
    "    extra = dict(persistent_workers=pw)\n",
    "    if NUM_WORKERS > 0:\n",
    "        extra.update(prefetch_factor=2)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fun, \n",
    "        num_workers=NUM_WORKERS, pin_memory=True, **extra\n",
    "    )\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    xb, xa, yb = next(iter(train_loader))\n",
    "    print(f\"Time to first batch: {time.time() - t0:.2f}s | batch shape: {tuple(xb.shape)}\")\n",
    "    del xb, xa, yb\n",
    "    \n",
    "    val_loader   = DataLoader(\n",
    "        val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fun, \n",
    "        num_workers=NUM_WORKERS, pin_memory=True, **extra\n",
    "    )\n",
    "\n",
    "    model = ExtendedModel(model_name, num_classes=3).to(device)\n",
    "    ''' NEW '''\n",
    "    ema = ModelEmaV2(model, decay=0.9999)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=1)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE, delta=0.001, verbose=True)\n",
    "    early_stop_path = CHECKPOINT_DIR / f\"{model_name}_earlystop_best.pth\"\n",
    "\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
    "\n",
    "    train_accs, val_accs, train_losses, val_losses = [], [], [], []\n",
    "\n",
    "    # === SAVING / CHECKPOINTING SETUP (paths derived from save_path; folder variables unchanged) ===\n",
    "    from pathlib import Path as _Path\n",
    "    _save_p = _Path(save_path)  # e.g., .../ind_study/data/results/models/tiny_vit_model/<name>.pth\n",
    "    # checkpoints dir: .../ind_study/data/results/checkpoints/tiny_vit_model/\n",
    "    _ckpt_dir = _save_p.parent.parent / \"checkpoints\" / _save_p.parent.name\n",
    "    _ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Track best EMA snapshot by lowest val_loss (per fold)\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        total_train_loss = 0.0\n",
    "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for images, aux, labels in progress:\n",
    "            images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mixup:\n",
    "                images, aux_features, targets_a, targets_b, lam = mixup_data(images, aux, labels, ALPHA_MIXUP)\n",
    "                mixup_mode = True\n",
    "            else:\n",
    "                aux_features = aux\n",
    "                mixup_mode = False\n",
    "\n",
    "            with autocast(device_type=str(device).split(':')[0] if device.type=='cuda' else 'cpu'):\n",
    "                outputs = model(images, aux_features)\n",
    "                if mixup_mode:\n",
    "                    loss = mixup_criterion(F.cross_entropy, outputs, targets_a, targets_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            '''  NEW  '''\n",
    "            ema.update(model)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            progress.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        model_for_eval = ema.module   # NEW\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, aux, labels in val_loader:\n",
    "                images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "                # outputs = model(images, aux) \n",
    "                outputs = model_for_eval(images, aux)  # NEW\n",
    "                total_val_loss += F.cross_entropy(outputs, labels).item()  # plain CE for validation\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc, val_f1, cm, _, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # === SAVE THIS EPOCH (ALL epochs, ALL folds) to results/checkpoints/tiny_vit_model/\n",
    "        torch.save(ema.module.state_dict(), _ckpt_dir / f\"{_save_p.stem}_epoch{epoch+1}.pth\")\n",
    "\n",
    "        # === UPDATE BEST (LOWEST val_loss) using EMA snapshot\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            # store on CPU to avoid device issues during CV loop\n",
    "            best_state = {k: v.detach().cpu() for k, v in ema.module.state_dict().items()}\n",
    "\n",
    "        early_stopper(val_loss, model, early_stop_path)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(epoch)\n",
    "        gc.collect()\n",
    "\n",
    "    # === FINALIZE: load the BEST EMA snapshot for this fold and save to models/tiny_vit_model (save_path)\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    else:\n",
    "        # fallback: current EMA if no best captured for some reason\n",
    "        model.load_state_dict(ema.module.state_dict())\n",
    "\n",
    "    # Save one BEST .pth per fold under .../results/models/tiny_vit_model/\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    return model, train_accs, val_accs, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2e3e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after dropna: (24922, 5)\n",
      "\n",
      "=== Fold 1 ===\n",
      "Time to first batch: 0.67s | batch shape: (32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 624/624 [06:38<00:00,  1.57it/s, acc=0.559, loss=0.669]\n",
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5615    1.0000    0.7192      2797\n",
      "           1     0.0000    0.0000    0.0000       952\n",
      "           2     0.0000    0.0000    0.0000      1232\n",
      "\n",
      "    accuracy                         0.5615      4981\n",
      "   macro avg     0.1872    0.3333    0.2397      4981\n",
      "weighted avg     0.3153    0.5615    0.4039      4981\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   8%|         | 49/624 [00:31<06:05,  1.57it/s, acc=0.571, loss=0.99] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m val_df.to_csv(val_fold_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     69\u001b[39m model_save_path = save_path / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m model, train_accs, val_accs, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fold_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_fold_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m plot_metrics(train_accs, val_accs, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, plot_save_path)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Use the same clean val transform you defined earlier\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_name, train_csv, val_csv, root_dir, save_path, device, use_mixup, label_smoothing)\u001b[39m\n\u001b[32m     94\u001b[39m total_train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     95\u001b[39m progress = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mSharedHeadDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     42\u001b[39m row = \u001b[38;5;28mself\u001b[39m.data.iloc[idx]\n\u001b[32m     43\u001b[39m img_path = \u001b[38;5;28mself\u001b[39m.root_dir / row[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# timm transform expects PIL -> tensor, normalized\u001b[39;00m\n\u001b[32m     48\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.transform_img(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\PIL\\Image.py:993\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    991\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    997\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 6: CV loop + saving folds + running train_model (matches text flow)  :contentReference[oaicite:5]{index=5}\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# drop rows missing required columns (text does this)\n",
    "df = df.dropna(subset=['image', 'label', 'brightness', 'edge_density', 'entropy']).reset_index(drop=True)\n",
    "\n",
    "print(\"Dataset size after dropna:\", df.shape)\n",
    "# skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "'''  NEW  '''\n",
    "def base_id(p: str) -> str:\n",
    "    s = Path(p).stem\n",
    "    return s.split('_trivialaug')[0]   # adjust if the augment suffix differs\n",
    "\n",
    "df['group'] = df['image'].apply(base_id)\n",
    "\n",
    "skf = StratifiedGroupKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "FOLDS_DIR = OUTPUT_DIR / \"folds\"\n",
    "FOLDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# select backbone per text (tiny_vit index 3 in text)\n",
    "model_configs = [\n",
    "    (\"swin_small_patch4_window7_224.ms_in1k\", \"swin_model.pth\"),\n",
    "    (\"coatnet_1_rw_224.sw_in1k\", \"coatnet_model.pth\"),\n",
    "    (\"convnext_small.fb_in1k\", \"convnext_model.pth\"),\n",
    "    (\"tiny_vit_5m_224.dist_in22k_ft_in1k\", \"tiny_vit_model.pth\"),\n",
    "    (\"edgenext_xx_small.in1k\", \"edgenext_xx_model.pth\"),\n",
    "]\n",
    "model_id = 3\n",
    "model_name = model_configs[model_id][0]\n",
    "history = model_configs[model_id][1]\n",
    "\n",
    "save_path = OUTPUT_DIR / \"models\" / history.split('.')[0]\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "plot_save_path = OUTPUT_DIR / \"figures\" / history.split('.')[0]\n",
    "plot_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
    "#     print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "#     train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "#     val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "#     train_fold_path = FOLDS_DIR / f\"{history}_train_fold_{fold+1}.csv\"\n",
    "#     val_fold_path   = FOLDS_DIR / f\"{history}_val_fold_{fold+1}.csv\"\n",
    "#     train_df.to_csv(train_fold_path, index=False)\n",
    "#     val_df.to_csv(val_fold_path, index=False)\n",
    "\n",
    "#     model_save_path = save_path / f\"{model_name}_fold{fold+1}.pth\"\n",
    "#     model, train_accs, val_accs, train_losses, val_losses = train_model(\n",
    "#         model_name, train_fold_path, val_fold_path, ROOT_DIR, model_save_path\n",
    "#     )\n",
    "\n",
    "#     plot_metrics(train_accs, val_accs, f\"{model_name}_fold{fold+1}\", plot_save_path)\n",
    "#     # Optionally show final confusion matrix\n",
    "#     _, _, cm, _, _ = evaluate_model(model, DataLoader(SharedHeadDataset(val_fold_path, ROOT_DIR),\n",
    "#                                                        batch_size=BATCH_SIZE, collate_fn=collate_fun), DEVICE)\n",
    "#     plot_confusion_matrix(cm, f\"{model_name}_fold{fold+1}\")\n",
    "\n",
    "'''  NEW - stratified group k-fold '''\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, y=df['label'], groups=df['group'])):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_fold_path = FOLDS_DIR / f\"{history}_train_fold_{fold+1}.csv\"\n",
    "    val_fold_path   = FOLDS_DIR / f\"{history}_val_fold_{fold+1}.csv\"\n",
    "    train_df.to_csv(train_fold_path, index=False)\n",
    "    val_df.to_csv(val_fold_path, index=False)\n",
    "\n",
    "    model_save_path = save_path / f\"{model_name}_fold{fold+1}.pth\"\n",
    "    model, train_accs, val_accs, train_losses, val_losses = train_model(\n",
    "        model_name, train_fold_path, val_fold_path, ROOT_DIR, model_save_path\n",
    "    )\n",
    "\n",
    "    plot_metrics(train_accs, val_accs, f\"{model_name}_fold{fold+1}\", plot_save_path)\n",
    "\n",
    "    # Use the same clean val transform you defined earlier\n",
    "    val_loader = DataLoader(\n",
    "        SharedHeadDataset(val_fold_path, ROOT_DIR, transform_img=_val_tfms),\n",
    "        batch_size=BATCH_SIZE, collate_fn=collate_fun\n",
    "    )\n",
    "    _, _, cm, _, _ = evaluate_model(model, val_loader, DEVICE)\n",
    "    plot_confusion_matrix(cm, f\"{model_name}_fold{fold+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384c698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

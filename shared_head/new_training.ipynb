{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c5eda7",
   "metadata": {},
   "source": [
    "### Training the new model with additional dataset - Larch Casebearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eca89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 24922\n",
      "Missing files: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# paths\n",
    "CSV_PATH = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\\overall_merged_images.csv\")\n",
    "BASE_DIR = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\")\n",
    "\n",
    "# load\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert \"image\" in df.columns, \"CSV must have a column named 'image'\"\n",
    "\n",
    "# build absolute paths\n",
    "df[\"abs_path\"] = df[\"image\"].apply(lambda p: (BASE_DIR / Path(str(p).replace(\"\\\\\",\"/\"))).resolve())\n",
    "df[\"exists\"] = df[\"abs_path\"].apply(lambda p: p.exists())\n",
    "\n",
    "# summary\n",
    "missing = df.loc[~df[\"exists\"], \"image\"]\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing files: {len(missing)}\")\n",
    "if len(missing) > 0:\n",
    "    print(\"Examples of missing paths:\")\n",
    "    print(missing.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb6d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe108595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOTS = {\n",
    "    \"vedai\": Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "    \"shha\":  Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "    \"shhb\":  Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "    \"larch\": Path(\"C:/Users/rsriram3/Documents/ind_study/data\"),\n",
    "}\n",
    "\n",
    "MODEL_NAME   = \"tiny_vit_5m_224.dist_in22k_ft_in1k\"\n",
    "IMG_SIZE     = 224\n",
    "NUM_CLASSES  = 3\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 10\n",
    "LR           = 2e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE     = 5                 # early stopping patience (epochs)\n",
    "LABEL_SMOOTH = 0.05              # label smoothing for CE\n",
    "SEED         = 123\n",
    "NUM_WORKERS  = 2                 # 0 on Windows if needed\n",
    "N_SPLITS     = 5                 # stratified K-fold (set to 1 for a single split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820f7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.cuda.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config, create_transform\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43af095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unresolved rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>brightness</th>\n",
       "      <th>edge_density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>abs_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VEDAI_dataset\\VEDAI_512\\images\\00000000_co.png</td>\n",
       "      <td>0</td>\n",
       "      <td>148.686729</td>\n",
       "      <td>0.132713</td>\n",
       "      <td>6.791240</td>\n",
       "      <td>C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VEDAI_dataset\\VEDAI_512\\images\\00000000_ir.png</td>\n",
       "      <td>0</td>\n",
       "      <td>194.854771</td>\n",
       "      <td>0.026806</td>\n",
       "      <td>5.643956</td>\n",
       "      <td>C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VEDAI_dataset\\VEDAI_512\\images\\00000000_ir_tri...</td>\n",
       "      <td>0</td>\n",
       "      <td>181.481246</td>\n",
       "      <td>0.041693</td>\n",
       "      <td>5.920810</td>\n",
       "      <td>C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label  brightness  \\\n",
       "0     VEDAI_dataset\\VEDAI_512\\images\\00000000_co.png      0  148.686729   \n",
       "1     VEDAI_dataset\\VEDAI_512\\images\\00000000_ir.png      0  194.854771   \n",
       "2  VEDAI_dataset\\VEDAI_512\\images\\00000000_ir_tri...      0  181.481246   \n",
       "\n",
       "   edge_density   entropy                                           abs_path  \n",
       "0      0.132713  6.791240  C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...  \n",
       "1      0.026806  5.643956  C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...  \n",
       "2      0.041693  5.920810  C:\\Users\\rsriram3\\Documents\\ind_study\\data\\VED...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"image\",\"label\"}.issubset(df.columns), \"CSV must have columns: image,label\"\n",
    "\n",
    "def which_ds(p: str) -> str:\n",
    "    s = str(p).lower().replace(\"\\\\\",\"/\")\n",
    "    if \"vedai_512\" in s or \"vedai_1024\" in s or \"/vedai\" in s:\n",
    "        return \"vedai\"\n",
    "    if \"shhb\" in s or \"/shb\" in s or \"shanghaitech data\\\\shhb\" in s.lower():\n",
    "        return \"shhb\"\n",
    "    if \"shha\" in s or \"/sha\" in s or \"shanghaitech data\\\\shha\" in s.lower():\n",
    "        return \"shha\"\n",
    "    if \"data_set_larch_casebearer\" in s or \"larch_casebearer\" in s or \"augmented_images\" in s or \"/larch/\" in s:\n",
    "        return \"larch\"\n",
    "    return \"vedai\"\n",
    "\n",
    "def resolve_abs(rel: str) -> Path | None:\n",
    "    rel_p = Path(str(rel).replace(\"\\\\\",\"/\"))\n",
    "    # try inferred root first\n",
    "    root = ROOTS.get(which_ds(rel_p.as_posix()), ROOTS[\"vedai\"])\n",
    "    cand = (root / rel_p)\n",
    "    if cand.exists(): return cand.resolve()\n",
    "    # last resort: try all roots\n",
    "    for r in ROOTS.values():\n",
    "        c = (r / rel_p)\n",
    "        if c.exists(): return c.resolve()\n",
    "    return None\n",
    "\n",
    "df[\"abs_path\"] = df[\"image\"].apply(resolve_abs)\n",
    "missing = df[\"abs_path\"].isna().sum()\n",
    "print(\"Unresolved rows:\", missing)\n",
    "if missing:\n",
    "    display(df[df[\"abs_path\"].isna()].head(10))\n",
    "df = df.dropna(subset=[\"abs_path\"]).reset_index(drop=True)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6f3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " label\n",
      "0    13983\n",
      "1     4791\n",
      "2     6148\n",
      "Name: count, dtype: int64\n",
      "Class ratios:\n",
      " label\n",
      "0    0.5611\n",
      "1    0.1922\n",
      "2    0.2467\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "counts = df[\"label\"].value_counts().sort_index()\n",
    "print(\"Class counts:\\n\", counts)\n",
    "print(\"Class ratios:\\n\", (counts / counts.sum()).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a40ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved input_size: (3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "_tmp = timm.create_model('tiny_vit_5m_224.dist_in22k_ft_in1k', pretrained=True, num_classes=NUM_CLASSES)\n",
    "data_cfg = resolve_data_config(model=_tmp)\n",
    "print(\"Resolved input_size:\", data_cfg.get(\"input_size\"))\n",
    "\n",
    "train_tfms = create_transform(**data_cfg, is_training=True)   # includes resize/crop/norm\n",
    "val_tfms   = create_transform(**data_cfg, is_training=False)  # resize + center crop + norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d06d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, transform=None):\n",
    "        self.f = frame.reset_index(drop=True)\n",
    "        self.t = transform\n",
    "    def __len__(self): return len(self.f)\n",
    "    def __getitem__(self, i):\n",
    "        p = Path(self.f.loc[i, \"abs_path\"])\n",
    "        y = int(self.f.loc[i, \"label\"])\n",
    "        x = Image.open(p).convert(\"RGB\")\n",
    "        if self.t: x = self.t(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99b5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=PATIENCE, mode=\"min\", min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.count = 0\n",
    "\n",
    "    def step(self, value) -> bool:\n",
    "        if self.best is None:\n",
    "            self.best = value\n",
    "            return False\n",
    "        improved = (value < self.best - self.min_delta) if self.mode == \"min\" else (value > self.best + self.min_delta)\n",
    "        if improved:\n",
    "            self.best = value\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "        return self.count > self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def00ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES)\n",
    "    return model.to(device)\n",
    "\n",
    "def make_loaders(df_train, df_val):\n",
    "    ds_tr = CSVDataset(df_train, transform=train_tfms)\n",
    "    ds_va = CSVDataset(df_val,   transform=val_tfms)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return dl_tr, dl_va\n",
    "\n",
    "def train_one_model(df_train, df_val, max_epochs=EPOCHS, run_name=\"run\"):\n",
    "    dl_tr, dl_va = make_loaders(df_train, df_val)\n",
    "    model = create_model()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    # Cosine schedule over epochs (per-step cosine also OK; keep simple here)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    es = EarlyStopper(patience=PATIENCE, mode=\"min\")\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    def run_epoch(loader, train=True):\n",
    "        if train: model.train()\n",
    "        else:     model.eval()\n",
    "        tot_loss, tot, correct = 0.0, 0, 0\n",
    "        pbar = tqdm(loader, leave=False)\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            with autocast(True):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            tot_loss += loss.item() * xb.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            tot += xb.size(0)\n",
    "        return tot_loss / max(tot,1), correct / max(tot,1)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        tr_loss, tr_acc = run_epoch(dl_tr, train=True)\n",
    "        va_loss, va_acc = run_epoch(dl_va, train=False)\n",
    "        scheduler.step()\n",
    "        print(f\"[{run_name}] epoch {epoch:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = { \"state_dict\": model.state_dict(),\n",
    "                           \"num_classes\": NUM_CLASSES,\n",
    "                           \"img_size\": IMG_SIZE }\n",
    "        if es.step(va_loss):\n",
    "            print(f\"Early stopping at epoch {epoch} (best val loss {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Restore best state for evaluation\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"state_dict\"])\n",
    "    return model, best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c7aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = 4        # try 2 if RAM is tight\n",
    "PREFETCH    = 2\n",
    "BATCH_SIZE  = 32       # adjust if VRAM limited\n",
    "MAX_TRAIN_STEPS = 200 # set e.g. 200 for quick feedback, else None\n",
    "MAX_VAL_STEPS   = 60 # set e.g. 60  for quick feedback, else None\n",
    "\n",
    "def make_loaders(df_train, df_val):\n",
    "    ds_tr = CSVDataset(df_train, transform=train_tfms)\n",
    "    ds_va = CSVDataset(df_val,   transform=val_tfms)\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=PREFETCH\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        ds_va, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=PREFETCH\n",
    "    )\n",
    "    return dl_tr, dl_va\n",
    "\n",
    "# One-line device sanity once you create the model in training:\n",
    "# print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 | train=19937 val=4985 ===\n",
      "train label counts: {0: 11186, 1: 3833, 2: 4918}\n",
      "val   label counts: {0: 2797, 1: 958, 2: 1230}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/624 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_ALL_FOLDS = False\n",
    "labels = df[\"label\"].values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_models = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):\n",
    "    df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "    df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n=== Fold {fold}/{N_SPLITS} | train={len(df_tr)} val={len(df_va)} ===\")\n",
    "    print(\"train label counts:\", df_tr[\"label\"].value_counts().sort_index().to_dict())\n",
    "    print(\"val   label counts:\", df_va[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "    model, best_state = train_one_model(df_tr, df_va, max_epochs=EPOCHS, run_name=f\"fold{fold}\")\n",
    "    fold_models.append((model, best_state))\n",
    "\n",
    "    if not TRAIN_ALL_FOLDS:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0ca47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0666e99",
   "metadata": {},
   "source": [
    "preivious k-fold training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_ALL_FOLDS = False  # set True to train all folds\n",
    "\n",
    "labels = df[\"label\"].values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_models = []\n",
    "if __name__ == \"__main__\":  # important on Windows to allow num_workers>0 in your dataloaders\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):\n",
    "        df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "        df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\n=== Fold {fold}/{N_SPLITS} | train={len(df_tr)} val={len(df_va)} ===\")\n",
    "        print(\"train label counts:\", df_tr[\"label\"].value_counts().sort_index().to_dict())\n",
    "        print(\"val   label counts:\", df_va[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "        model, best_state = train_one_model(df_tr, df_va, max_epochs=EPOCHS, run_name=f\"fold{fold}\")\n",
    "        fold_models.append((model, best_state))\n",
    "\n",
    "        if not TRAIN_ALL_FOLDS:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 | train=19937 val=4985 ===\n",
      "train label counts: {0: 11186, 1: 3833, 2: 4918}\n",
      "val   label counts: {0: 2797, 1: 958, 2: 1230}\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/624 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import numpy as np\n",
    "# from torch.amp import GradScaler\n",
    "# from torch import nn, autocast\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# TRAIN_ALL_FOLDS = False  # True = train all folds; False = first fold only\n",
    "# labels = df[\"label\"].values\n",
    "# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# def train_one_model(df_train, df_val, max_epochs=EPOCHS, run_name=\"run\"):\n",
    "#     dl_tr, dl_va = make_loaders(df_train, df_val)\n",
    "#     model = create_model()\n",
    "#     print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "#     scaler = GradScaler()\n",
    "#     es = EarlyStopper(patience=PATIENCE, mode=\"min\")\n",
    "#     best_val = float(\"inf\")\n",
    "#     best_state = None\n",
    "\n",
    "#     def run_epoch(loader, train=True):\n",
    "#         model.train(train)\n",
    "#         total_loss, correct, total = 0.0, 0, 0\n",
    "#         step = 0\n",
    "#         pbar = tqdm(loader, leave=False)\n",
    "#         for xb, yb in pbar:\n",
    "#             step += 1\n",
    "#             xb = xb.to(\"cuda\", non_blocking=True) if torch.cuda.is_available() else xb\n",
    "#             yb = yb.to(\"cuda\", non_blocking=True) if torch.cuda.is_available() else yb\n",
    "#             with autocast(True):\n",
    "#                 logits = model(xb)\n",
    "#                 loss = criterion(logits, yb)\n",
    "#             if train:\n",
    "#                 optimizer.zero_grad(set_to_none=True)\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#             total_loss += loss.item() * xb.size(0)\n",
    "#             correct += (logits.argmax(1) == yb).sum().item()\n",
    "#             total   += xb.size(0)\n",
    "#             if train and MAX_TRAIN_STEPS and step >= MAX_TRAIN_STEPS: break\n",
    "#             if not train and MAX_VAL_STEPS and step >= MAX_VAL_STEPS: break\n",
    "#         return total_loss / max(total,1), correct / max(total,1)\n",
    "\n",
    "#     for epoch in range(1, max_epochs+1):\n",
    "#         tr_loss, tr_acc = run_epoch(dl_tr, train=True)\n",
    "#         va_loss, va_acc = run_epoch(dl_va, train=False)\n",
    "#         scheduler.step()\n",
    "#         print(f\"[{run_name}] epoch {epoch:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "#         if va_loss < best_val:\n",
    "#             best_val = va_loss\n",
    "#             best_state = {\"state_dict\": model.state_dict(), \"num_classes\": NUM_CLASSES, \"img_size\": IMG_SIZE}\n",
    "#         if es.step(va_loss):\n",
    "#             print(f\"Early stopping at epoch {epoch} (best val loss {best_val:.4f})\")\n",
    "#             break\n",
    "\n",
    "#     if best_state is not None:\n",
    "#         model.load_state_dict(best_state[\"state_dict\"])\n",
    "#     return model, best_state\n",
    "\n",
    "# fold_models = []\n",
    "# if __name__ == \"__main__\":\n",
    "#     for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):\n",
    "#         df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "#         df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "#         print(f\"\\n=== Fold {fold}/{N_SPLITS} | train={len(df_tr)} val={len(df_va)} ===\")\n",
    "#         print(\"train label counts:\", df_tr[\"label\"].value_counts().sort_index().to_dict())\n",
    "#         print(\"val   label counts:\", df_va[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "#         model, best_state = train_one_model(df_tr, df_va, max_epochs=EPOCHS, run_name=f\"fold{fold}\")\n",
    "#         fold_models.append((model, best_state))\n",
    "\n",
    "#         if not TRAIN_ALL_FOLDS:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903daabc",
   "metadata": {},
   "source": [
    "### COMPLETElY CHANGING THE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf35667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f032e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CSV: C:\\Users\\rsriram3\\Documents\\ind_study\\data\\overall_merged_images.csv\n",
      "ROOT_DIR: C:\\Users\\rsriram3\\Documents\\ind_study\\data\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: config & imports (edit paths to match your environment)\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autocast\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "import random\n",
    "\n",
    "# === EDIT THESE PATHS ===\n",
    "CSV_PATH = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\\overall_merged_images.csv\")# your merged csv\n",
    "ROOT_DIR   = Path(r\"C:\\Users\\rsriram3\\Documents\\ind_study\\data\")  # base path to prepend to 'image' column entries\n",
    "OUTPUT_DIR = ROOT_DIR / \"results\" # where to put models/figures/folds\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === HYPERPARAMS (as in text file) ===\n",
    "BATCH_SIZE    = 32\n",
    "EPOCHS        = 5\n",
    "LR            = 3e-5\n",
    "WEIGHT_DECAY  = 5e-3\n",
    "ALPHA_MIXUP   = 0.2\n",
    "USE_MIXUP     = False           # set True to enable mixup (matches text flow)\n",
    "NUM_FOLDS     = 5\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENCE      = 3               # early stopping patience used in text\n",
    "LABEL_SMOOTH  = 0.1\n",
    "SEED          = 173\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CSV:\", CSV_PATH)\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359cdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: dataset, collate, mixup (copied/adjusted from test_text.txt)  :contentReference[oaicite:1]{index=1}\n",
    "import torch\n",
    "\n",
    "class SharedHeadDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = torch.nn.Sequential()  # placeholder; we will use PIL->tensor manually\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = self.root_dir / row['image']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # Resize + ToTensor + Normalize: use simple resize & to-tensor here (the original text used Resize->ToTensor)\n",
    "        image = image.resize((224, 224))\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2,0,1).contiguous()  # C,H,W\n",
    "\n",
    "        aux = torch.tensor([\n",
    "            float(row['brightness']), \n",
    "            float(row['edge_density']), \n",
    "            float(row['entropy'])\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(int(row['label']), dtype=torch.long)\n",
    "        return image, aux, label\n",
    "\n",
    "def collate_fun(batch):\n",
    "    images, auxs, labels = zip(*batch)\n",
    "    images = torch.stack(images)    # (B,3,224,224)\n",
    "    auxs   = torch.stack(auxs)      # (B,3)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return images, auxs, labels\n",
    "\n",
    "def mixup_data(x, aux, y, alpha=ALPHA_MIXUP):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_aux = lam * aux + (1 - lam) * aux[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, mixed_aux, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e86023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: ExtendedModel + LabelSmoothingCrossEntropy (from text)  :contentReference[oaicite:2]{index=2}\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(self.smoothing / (pred.size(1) - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "# ExtendedModel: uses timm backbone, aux head, fused head (as in test_text)\n",
    "class ExtendedModel(nn.Module):\n",
    "    def __init__(self, backbone_name, num_classes=3):   # set 3 classes for your new dataset\n",
    "        super().__init__()\n",
    "        self.backbone = create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(self.backbone.num_features + 16),\n",
    "            nn.Linear(self.backbone.num_features + 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, aux):\n",
    "        # x: (B,3,H,W)\n",
    "        image_features = self.backbone(x)  # (B, D)\n",
    "        aux_features = self.aux_head(aux)  # (B, 16)\n",
    "        combined = torch.cat([image_features, aux_features], dim=1)\n",
    "        return self.head(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daa1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: eval, plotting and EarlyStopping (copied from text)  :contentReference[oaicite:3]{index=3}\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, aux, labels in dataloader:\n",
    "            images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "            outputs = model(images, aux)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    return acc, f1, cm, all_preds, all_labels\n",
    "\n",
    "def plot_metrics(train_accs, val_accs, model_name, save_path):\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(train_accs, label=\"Train Acc\")\n",
    "    plt.plot(val_accs, label=\"Val Acc\")\n",
    "    plt.title(f\"Accuracy vs Epoch - {model_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path / f\"{model_name}_accuracy.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, show=True):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EarlyStopping + train_model (matches test_text.txt training loop)  :contentReference[oaicite:4]{index=4}\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.001, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None or score > self.best_score + self.delta:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# train_model: uses ExtendedModel, optional mixup, label smoothing CE, GradScaler, scheduler, early stopping\n",
    "def train_model(model_name, train_csv, val_csv, root_dir, save_path,\n",
    "                device=DEVICE, use_mixup=USE_MIXUP, label_smoothing=LABEL_SMOOTH):\n",
    "    train_ds = SharedHeadDataset(train_csv, root_dir)\n",
    "    val_ds   = SharedHeadDataset(val_csv, root_dir)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fun)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, collate_fn=collate_fun)\n",
    "\n",
    "    model = ExtendedModel(model_name, num_classes=3).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=1)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE, delta=0.001, verbose=True)\n",
    "    early_stop_path = CHECKPOINT_DIR / f\"{model_name}_earlystop_best.pth\"\n",
    "\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
    "\n",
    "    train_accs, val_accs, train_losses, val_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        total_train_loss = 0.0\n",
    "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for images, aux, labels in progress:\n",
    "            images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mixup:\n",
    "                images, aux_features, targets_a, targets_b, lam = mixup_data(images, aux, labels, ALPHA_MIXUP)\n",
    "                mixup_mode = True\n",
    "            else:\n",
    "                aux_features = aux\n",
    "                mixup_mode = False\n",
    "\n",
    "            with autocast(device_type=str(device).split(':')[0] if device.type=='cuda' else 'cpu'):\n",
    "                outputs = model(images, aux_features)\n",
    "                if mixup_mode:\n",
    "                    loss = mixup_criterion(F.cross_entropy, outputs, targets_a, targets_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            progress.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, aux, labels in val_loader:\n",
    "                images, aux, labels = images.to(device), aux.to(device), labels.to(device)\n",
    "                outputs = model(images, aux)\n",
    "                total_val_loss += F.cross_entropy(outputs, labels).item()  # plain CE for validation\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc, val_f1, cm, _, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        early_stopper(val_loss, model, early_stop_path)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(epoch)\n",
    "        gc.collect()\n",
    "\n",
    "    # Save last state and reload best if early stopped\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    if early_stopper.early_stop:\n",
    "        model.load_state_dict(torch.load(early_stop_path))\n",
    "\n",
    "    return model, train_accs, val_accs, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2e3e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after dropna: (24922, 5)\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 624/624 [09:33<00:00,  1.09it/s, acc=0.876, loss=2.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2797\n",
      "           1     1.0000    1.0000    1.0000       958\n",
      "           2     1.0000    1.0000    1.0000      1230\n",
      "\n",
      "    accuracy                         1.0000      4985\n",
      "   macro avg     1.0000    1.0000    1.0000      4985\n",
      "weighted avg     1.0000    1.0000    1.0000      4985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 624/624 [07:02<00:00,  1.48it/s, acc=0.999, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2797\n",
      "           1     1.0000    1.0000    1.0000       958\n",
      "           2     1.0000    1.0000    1.0000      1230\n",
      "\n",
      "    accuracy                         1.0000      4985\n",
      "   macro avg     1.0000    1.0000    1.0000      4985\n",
      "weighted avg     1.0000    1.0000    1.0000      4985\n",
      "\n",
      "EarlyStopping: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 624/624 [07:04<00:00,  1.47it/s, acc=0.999, loss=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2797\n",
      "           1     1.0000    1.0000    1.0000       958\n",
      "           2     1.0000    1.0000    1.0000      1230\n",
      "\n",
      "    accuracy                         1.0000      4985\n",
      "   macro avg     1.0000    1.0000    1.0000      4985\n",
      "weighted avg     1.0000    1.0000    1.0000      4985\n",
      "\n",
      "EarlyStopping: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 624/624 [07:02<00:00,  1.48it/s, acc=1, loss=2.75] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2797\n",
      "           1     1.0000    1.0000    1.0000       958\n",
      "           2     1.0000    1.0000    1.0000      1230\n",
      "\n",
      "    accuracy                         1.0000      4985\n",
      "   macro avg     1.0000    1.0000    1.0000      4985\n",
      "weighted avg     1.0000    1.0000    1.0000      4985\n",
      "\n",
      "EarlyStopping: 3/3\n",
      "Early stopping triggered.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2797\n",
      "           1     1.0000    1.0000    1.0000       958\n",
      "           2     1.0000    1.0000    1.0000      1230\n",
      "\n",
      "    accuracy                         1.0000      4985\n",
      "   macro avg     1.0000    1.0000    1.0000      4985\n",
      "weighted avg     1.0000    1.0000    1.0000      4985\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 624/624 [09:16<00:00,  1.12it/s, acc=0.899, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9996    0.9996      2797\n",
      "           1     1.0000    0.9990    0.9995       958\n",
      "           2     0.9992    1.0000    0.9996      1230\n",
      "\n",
      "    accuracy                         0.9996      4985\n",
      "   macro avg     0.9996    0.9995    0.9996      4985\n",
      "weighted avg     0.9996    0.9996    0.9996      4985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 624/624 [06:56<00:00,  1.50it/s, acc=0.999, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    1.0000    0.9998      2797\n",
      "           1     1.0000    0.9990    0.9995       958\n",
      "           2     1.0000    1.0000    1.0000      1230\n",
      "\n",
      "    accuracy                         0.9998      4985\n",
      "   macro avg     0.9999    0.9997    0.9998      4985\n",
      "weighted avg     0.9998    0.9998    0.9998      4985\n",
      "\n",
      "EarlyStopping: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  88%|████████▊ | 548/624 [06:00<00:49,  1.52it/s, acc=1, loss=0.428]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m val_df.to_csv(val_fold_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m model_save_path = save_path / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m model, train_accs, val_accs, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fold_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_fold_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m plot_metrics(train_accs, val_accs, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, plot_save_path)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Optionally show final confusion matrix\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_name, train_csv, val_csv, root_dir, save_path, device, use_mixup, label_smoothing)\u001b[39m\n\u001b[32m     52\u001b[39m total_train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     53\u001b[39m progress = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mSharedHeadDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     14\u001b[39m row = \u001b[38;5;28mself\u001b[39m.data.iloc[idx]\n\u001b[32m     15\u001b[39m img_path = \u001b[38;5;28mself\u001b[39m.root_dir / row[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Resize + ToTensor + Normalize: use simple resize & to-tensor here (the original text used Resize->ToTensor)\u001b[39;00m\n\u001b[32m     18\u001b[39m image = image.resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\PIL\\Image.py:1005\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m   1003\u001b[39m         mode = \u001b[33m\"\u001b[39m\u001b[33mRGBA\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m (mode == \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matrix):\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[32m   1008\u001b[39m     \u001b[38;5;66;03m# matrix conversion\u001b[39;00m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rsriram3\\Documents\\ind_study\\test-IIM\\shenv\\Lib\\site-packages\\PIL\\Image.py:1283\u001b[39m, in \u001b[36mImage.copy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1276\u001b[39m \u001b[33;03mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[32m   1277\u001b[39m \u001b[33;03minto an image, but still retain the original.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1280\u001b[39m \u001b[33;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[32m   1281\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28mself\u001b[39m.load()\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 6: CV loop + saving folds + running train_model (matches text flow)  :contentReference[oaicite:5]{index=5}\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# drop rows missing required columns (text does this)\n",
    "df = df.dropna(subset=['image', 'label', 'brightness', 'edge_density', 'entropy']).reset_index(drop=True)\n",
    "\n",
    "print(\"Dataset size after dropna:\", df.shape)\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "FOLDS_DIR = OUTPUT_DIR / \"folds\"\n",
    "FOLDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# select backbone per text (tiny_vit index 3 in text)\n",
    "model_configs = [\n",
    "    (\"swin_small_patch4_window7_224.ms_in1k\", \"swin_model.pth\"),\n",
    "    (\"coatnet_1_rw_224.sw_in1k\", \"coatnet_model.pth\"),\n",
    "    (\"convnext_small.fb_in1k\", \"convnext_model.pth\"),\n",
    "    (\"tiny_vit_5m_224.dist_in22k_ft_in1k\", \"tiny_vit_model.pth\"),\n",
    "    (\"edgenext_xx_small.in1k\", \"edgenext_xx_model.pth\"),\n",
    "]\n",
    "model_id = 3\n",
    "model_name = model_configs[model_id][0]\n",
    "history = model_configs[model_id][1]\n",
    "\n",
    "save_path = OUTPUT_DIR / \"models\" / history.split('.')[0]\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "plot_save_path = OUTPUT_DIR / \"figures\" / history.split('.')[0]\n",
    "plot_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_fold_path = FOLDS_DIR / f\"{history}_train_fold_{fold+1}.csv\"\n",
    "    val_fold_path   = FOLDS_DIR / f\"{history}_val_fold_{fold+1}.csv\"\n",
    "    train_df.to_csv(train_fold_path, index=False)\n",
    "    val_df.to_csv(val_fold_path, index=False)\n",
    "\n",
    "    model_save_path = save_path / f\"{model_name}_fold{fold+1}.pth\"\n",
    "    model, train_accs, val_accs, train_losses, val_losses = train_model(\n",
    "        model_name, train_fold_path, val_fold_path, ROOT_DIR, model_save_path\n",
    "    )\n",
    "\n",
    "    plot_metrics(train_accs, val_accs, f\"{model_name}_fold{fold+1}\", plot_save_path)\n",
    "    # Optionally show final confusion matrix\n",
    "    _, _, cm, _, _ = evaluate_model(model, DataLoader(SharedHeadDataset(val_fold_path, ROOT_DIR),\n",
    "                                                       batch_size=BATCH_SIZE, collate_fn=collate_fun), DEVICE)\n",
    "    plot_confusion_matrix(cm, f\"{model_name}_fold{fold+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf2bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
